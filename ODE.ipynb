{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Ordinary Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an ODE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vertical Projectile motion:\n",
    "\n",
    "\n",
    "$$\\sum{F} = \\frac{dp}{dt} $$ \n",
    "\n",
    "$$- (F_{drag} + F_{grav}) = \\frac{dp}{dt} $$ \n",
    "\n",
    "$$ -(k \\frac{dy}{dt} + m g) = m \\frac{d^2y}{dt^2} $$ \n",
    "    \n",
    "By integrating both sides from $ t_0 $ to $ t_0 + t $:\n",
    "        \n",
    "$$ -(k y + m g t) = m \\frac{dy}{dt} $$ \n",
    "\n",
    "$$ \\frac{dy}{dt} = \\frac{-(k y + m g t)}{m} \\tag{1.1}$$ \n",
    "$$ \\frac{dy}{dt} = f(y, t) \\tag{1.2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of numerically solving $(1.2)$, would be by integrating in $t$ with discrete steps of $t$:\n",
    "\n",
    "$$ \\frac{dy}{dt} = f(y, t) \\tag{1.2}$$ \n",
    "\n",
    "$$y(t_0 + \\Delta t) - y(t_0) = \\Delta t   f(y(t_0), t)$$\n",
    "\n",
    "$$y(t_0 + 2\\Delta t) - y(t_0 + \\Delta t) = \\Delta t   f(y(t_0 + \\Delta t), t)$$\n",
    "\n",
    "$$y(t_0 + 3\\Delta t) - y(t_0 + 2\\Delta t) = \\Delta t   f(y(t_0 + 2\\Delta t), t)$$\n",
    "\n",
    "\n",
    "$$...$$\n",
    "\n",
    "\n",
    "$$ y(t_n + \\Delta t)  = \\Delta t f(y(t_n), t) + y(t_n) \\tag{2}$$\n",
    "\n",
    "\n",
    "* The previous steps are known as `Euler's method`.\n",
    "\n",
    "* It's known for some time, that that $(2)$ resembles very much with the characteristic equation of `ResNets`, where $f(y_n, t)$ represents the output of a layer $n$ given $y_n$ as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classic ResNet example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ResNets had the best accuracy in the ImageNet Competition (2015)\n",
    "- ResNets uses skip-connections between layers, so the \"depth\" of the network becomes a feature to be learnt \n",
    "- Can have up to 100 layers while avoiding vanishing gradients.\n",
    "<img src=\"assets/1.png\">\n",
    "src: https://arxiv.org/abs/1512.03385\n",
    "\n",
    "### How it works?\n",
    "\n",
    "* Instead of using feeding the output of a previous layer into the next layer:\n",
    "$$\n",
    "f(z(t-1),\\ \\theta(t-1)) = z(t) $$\n",
    "\n",
    "$$\n",
    "f(z(t),\\ \\theta(t)) = z(t+1)\n",
    "$$\n",
    "\n",
    "* We feed the input of the previous layer as well:\n",
    "$$\n",
    "f(z(t-1),\\ \\theta(t-1)) + z(t-1)= z(t) $$\n",
    "\n",
    "$$\n",
    "f(z(t),\\ \\theta(t)) + z(t)= z(t+1)\n",
    "$$\n",
    "\n",
    "* Thus, if the network with $\\theta$ as parameters is trained with a set of measurements $\\{(z_0, t_0),(z_1, t_1),...,(z_M, t_M)\\}$, it will approximate the dynamics function $f(\\theta, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Ordinary Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given the noisy measurements:\n",
    "\n",
    "\n",
    "$$\\{(z_0, t_0),(z_1, t_1),...,(z_M, t_M)\\}$$\n",
    "\n",
    "\n",
    "* One wants to find an approximation $\\hat{f}(z, t, \\theta)$ for the dynamics of $f(z, t)$\n",
    "\n",
    "\n",
    "* Given $(z_0, t_0), (z_1, t_1)$, The system begin evolving and from $z_0, t_0$ until it reaches $z_1, t_0$. \n",
    "\n",
    "\n",
    "* One approximation ($\\hat{z_1}, t_1$) of the $\\hat{f}(z, t, \\theta)$ would be achieved through an integration of $(1)$\n",
    "\n",
    "\n",
    "$$\\Big( \\int_{t_0}^{t_1} f(z(t), t, \\theta)dt = \\hat{z}(t_1)\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the case where a optimizer wants to approximate $\\hat{z}(t)$, a cost function would be defined:\n",
    "$$L(z(t_1)) = L \\Big( \\int_{t_0}^{t_1} f(z(t), t, \\theta)dt \\Big) = L \\big( \\text{ODESolve}(z(t_0), f, t_0, t_1, \\theta) \\big) \\tag{2}$$\n",
    "\n",
    "\n",
    "* One main difference between optimizing $\\theta$ so $\\hat{f}(z, t, \\theta)$ can approximate the dynamics and the mere backpropagation of the $L(z(t_1))$ through a discrete number of layers, is that:\n",
    "\n",
    "\n",
    "    - It will be used only one \"layer-like network\" with the time as parameter.\n",
    "    \n",
    "    - Backpropagation will have to propagate over infinite time-steps in order to reach the initial condition $z_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to optimize $L$ one needs to compute the gradients wrt. its parameters: $z(t_0), t_0, t_1, \\theta$. \n",
    "\n",
    "$$ \\frac{\\partial L(z(t_1))}{\\partial \\theta}$$\n",
    "\n",
    "* But, by the chain-rule, one can pretty much lose itself:\n",
    "\n",
    "$$ \\frac{\\partial L(z(t_1))}{\\partial \\theta} = \\frac{\\partial L(z(t_1))}{\\partial z(t_1)} * \n",
    "\\frac{\\partial z(t_1)}{\\partial z(t_1 - \\delta t)} * \\frac{\\partial z(t_1 - \\delta t)}{\\partial z(t_1 - 2\\delta t)} ...\n",
    "\\frac{\\partial z(t_0)}{\\partial \\theta}$$\n",
    "\n",
    "* Thus, for optimizing $\\theta$ one should backpropagate the loss over every $t$ and be able to calculate the product of the every intermediate state. So, Better start by calculating what the paper calls *adjoint state*.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z(t)} = - \\alpha (t) $$\n",
    "\n",
    "\n",
    "The former is used to approximate the gradient of the loss w.r.t. the state $z$ at any time.\n",
    "\n",
    "\n",
    "* How the loss depends on the state at every moment of time $(z(t))$:\n",
    "\n",
    "\n",
    "$$ a(t) = -\\frac{\\partial L}{\\partial z(t)} \\tag{3} $$\n",
    "$a(t)$ is called *adjoint*, its dynamics is given by another ODE, which can be thought of as an instantaneous analog of the chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
