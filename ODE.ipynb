{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Ordinary Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an ODE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vertical Projectile motion:\n",
    "\n",
    "\n",
    "$$\\sum{F} = \\frac{dp}{dt} $$ \n",
    "\n",
    "$$- (F_{drag} + F_{grav}) = \\frac{dp}{dt} $$ \n",
    "\n",
    "$$ -(k \\frac{dy}{dt} + m g) = m \\frac{d^2y}{dt^2} $$ \n",
    "    \n",
    "By integrating both sides from $ t_0 $ to $ t_0 + t $:\n",
    "        \n",
    "$$ -(k y + m g t) = m \\frac{dy}{dt} $$ \n",
    "\n",
    "$$ \\frac{dy}{dt} = \\frac{-(k y + m g t)}{m} \\tag{1.1}$$ \n",
    "$$ \\frac{dy}{dt} = f(y, t) \\tag{1.2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of numerically solving $(1.2)$, would be by integrating in $t$ with discrete steps of $t$:\n",
    "\n",
    "$$ \\frac{dy}{dt} = f(y, t) \\tag{1.2}$$ \n",
    "\n",
    "$$y(t_0 + \\Delta t) - y(t_0) = \\Delta t   f(y(t_0), t)$$\n",
    "\n",
    "$$y(t_0 + 2\\Delta t) - y(t_0 + \\Delta t) = \\Delta t   f(y(t_0 + \\Delta t), t)$$\n",
    "\n",
    "$$y(t_0 + 3\\Delta t) - y(t_0 + 2\\Delta t) = \\Delta t   f(y(t_0 + 2\\Delta t), t)$$\n",
    "\n",
    "\n",
    "$$...$$\n",
    "\n",
    "\n",
    "$$ y(t_n + \\Delta t)  = \\Delta t f(y(t_n), t) + y(t_n) \\tag{1.3}$$\n",
    "\n",
    "\n",
    "* The previous steps are known as `Euler's method`.\n",
    "\n",
    "* It's known for some time, that that $(1.3)$ resembles very much with the characteristic equation of `ResNets`, where $f(y_n, t)$ represents the output of a layer $n$ given $y_n$ as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classic ResNet example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ResNets had the best accuracy in the ImageNet Competition (2015)\n",
    "- ResNets uses skip-connections between layers, so the \"depth\" of the network becomes a feature to be learnt \n",
    "- Can have up to 100 layers while avoiding vanishing gradients.\n",
    "<img src=\"assets/1.png\">\n",
    "src: https://arxiv.org/abs/1512.03385\n",
    "\n",
    "### How it works?\n",
    "\n",
    "* Instead of using feeding the output of a previous layer into the next layer:\n",
    "$$\n",
    "f(z(t-1),\\ \\theta(t-1)) = z(t) $$\n",
    "\n",
    "$$\n",
    "f(z(t),\\ \\theta(t)) = z(t+1)\n",
    "$$\n",
    "\n",
    "* We feed the input of the previous layer as well:\n",
    "$$\n",
    "f(z(t-1),\\ \\theta(t-1)) + z(t-1)= z(t) $$\n",
    "\n",
    "$$\n",
    "f(z(t),\\ \\theta(t)) + z(t)= z(t+1)\n",
    "$$\n",
    "\n",
    "* Thus, if the network with $\\theta$ as parameters is trained with a set of measurements $\\{(z_0, t_0),(z_1, t_1),...,(z_M, t_M)\\}$, it will approximate the dynamics function $f(\\theta, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Ordinary Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given the noisy measurements:\n",
    "\n",
    "\n",
    "$$\\{(z_0, t_0),(z_1, t_1),...,(z_M, t_M)\\}$$\n",
    "\n",
    "\n",
    "* One wants to find an approximation $\\hat{f}(z, t, \\theta)$ for the dynamics of $f(z, t)$ where\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial t}  = f(z(t), t, \\theta)$$ or even $$z(t + \\delta t) = \\int_{t}^{t + \\delta t} f(z(t), t, \\theta)dt + z(t) \\tag{2}$$\n",
    "\n",
    "\n",
    "* Given $(z_0, t_0), (z_1, t_1)$, The system begin evolving and from $z_0, t_0$ until it reaches $z_1, t_0$. \n",
    "\n",
    "\n",
    "* One approximation ($\\hat{z_1}, t_1$) of the $\\hat{f}(z, t, \\theta)$ would be achieved through an integration of $(1)$\n",
    "\n",
    "\n",
    "$$\\Big( \\int_{t_0}^{t_1} f(z(t), t, \\theta)dt = \\hat{z}(t_1)\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the case where a optimizer wants to approximate $\\hat{z}(t)$, a cost function would be defined:\n",
    "$$L(z(t_1)) = L \\Big( \\int_{t_0}^{t_1} f(z(t), t, \\theta)dt \\Big) = L \\big( \\text{ODESolve}(z(t_0), f, t_0, t_1, \\theta) \\big)$$\n",
    "\n",
    "\n",
    "* One main difference between optimizing $\\theta$ so $\\hat{f}(z, t, \\theta)$ can approximate the dynamics and the mere backpropagation of the $L(z(t_1))$ through a discrete number of layers, is that:\n",
    "\n",
    "\n",
    "    - It will be used only one \"layer-like network\" with the time as parameter (or the same weight distributed over time).\n",
    "    \n",
    "    - Backpropagation will have to propagate over infinite time-steps in order to reach the initial condition $z_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to optimize $L$ one needs to compute the gradients wrt. its parameters: $z(t_0), t_0, t_1, \\theta$. \n",
    "\n",
    "$$ \\frac{\\partial L(z(t_1))}{\\partial \\theta}$$\n",
    "\n",
    "* But, by the chain-rule, one can pretty much lose get lost:\n",
    "\n",
    "$$ \\frac{\\partial L(z(t_1))}{\\partial \\theta} = \\frac{\\partial L(z(t_1))}{\\partial z(t_1)} * \n",
    "\\frac{\\partial z(t_1)}{\\partial z(t_1 - \\delta t)} * \\frac{\\partial z(t_1 - \\delta t)}{\\partial z(t_1 - 2\\delta t)} ...\n",
    "\\frac{\\partial z(t_0)}{\\partial \\theta}$$\n",
    "\n",
    "* Thus, for optimizing $\\theta$ one should backpropagate the loss over every $t$ and be able to calculate the product of the every intermediate state. \n",
    "\n",
    "* So instead, the paper uses the *adjoint method* for computing the gradients by solving a different (easier, despite the appearances) problem by solving 3 different ODE's.\n",
    "$$\\frac{\\partial L}{\\partial \\theta} = \\int_{t_1}^{t_0} a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} dt \\tag{2.1}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z(t)} = -\\int_{t_1}^{t_0} a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial z} dt\\tag{2.2}$$\n",
    "\n",
    "$$z(t_1)=-\\int_{t_1}^{t_0} f(z(t), t, \\theta) dt\\tag{2.3}$$\n",
    "\n",
    "A simple/intuitive derivation of all those equations goes below.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjoint method (continuous backpropagation in residual networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First equation derivation (equation $2.1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consider the following 3-steps approximation of a n-ODE-like.\n",
    "\n",
    "<img src=\"assets/3-steps.png\">\n",
    "\n",
    "* We want to backpropagate the loss $L(z_3)$ through the network in order to optimize its weights.\n",
    "\n",
    "* Given that we got shared weights between all timesteps:\n",
    "\n",
    "$$ \\frac{\\partial L(z_3)}{\\partial \\theta} = L(z_2) + L(z_1) + L(z_0)$$\n",
    "\n",
    "$$= \\\\\n",
    "\\frac{\\partial L}{\\partial z_3}\\frac{\\partial z_3}{\\partial f(z_2)}\\frac{\\partial f(z_2)}{\\partial \\theta} + \\\\\n",
    "\\frac{\\partial L}{\\partial z_2}\\frac{\\partial z_2}{\\partial f(z_1)}\\frac{\\partial f(z_1)}{\\partial \\theta} + \\\\\n",
    "\\frac{\\partial L}{\\partial z_1}\\frac{\\partial z_1}{\\partial f(z_0)}\\frac{\\partial f(z_0)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta} = \\sum_{n=t_0}^{t_2} \\frac{\\partial L}{\\partial z(t_n)} \\frac{\\partial f(z_n)}{\\partial \\theta}$$\n",
    "\n",
    "* Extending the summation to an integral (approaches to infinite timesteps of infinitely small durations) and considering $t_3 = t_{final}=t_{1}$ \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta} = \\int_{t_1}^{t_0} \\frac{\\partial L}{\\partial z(t)} \\frac{\\partial f(z, t)}{\\partial \\theta} \\tag {3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second equation derivation (equation $2.2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the first term inside the integral in $(3)$, it is needed a way of calculating the gradient of the loss w.r.t. the state $z$ at any time given.\n",
    "$$ \\frac{\\partial L}{\\partial z(t)} $$ \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z(t)} = \\frac{\\partial L}{\\partial z(t + \\delta t)}\\frac{\\partial z(t+\\delta t)}{\\partial z(t)}$$ \n",
    "\n",
    "We can use the equation $(2)$ and replace it in $\\partial z(t + \\delta t)$. It yields:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z(t)} = \\frac{\\partial L}{\\partial z(t + \\delta t)}\n",
    "\\frac{\\partial (\\int_{t}^{t + \\delta t} f(z(t), t, \\theta)dt + z(t))}{\\partial z(t)}$$ \n",
    "\n",
    "I order to simplify the notation, we can call $\\frac{\\partial L}{\\partial z(t)} = \\alpha(t)$ (this is called *adjoint state*), thus:\n",
    "\n",
    "$$ \\alpha(t) = \\alpha(t + \\delta t)\n",
    "\\frac{\\partial (\\int_{t}^{t + \\delta t} f(z(t), t, \\theta)dt + z(t))}{\\partial z(t)}$$ \n",
    "\n",
    "The integral can be approximated by a *Talor Series* of 1st order around z(t): $\\int_{t}^{t + \\delta t} f(z(t), t, \\theta)dt + z(t) \\approx z(t) + \\delta t f(z(t), t, \\theta )$. Using it in the previous equation:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\alpha(t) = \\alpha(t + \\delta t)  + \\alpha(t + \\delta t) \\delta t \\frac{\\partial f(z(t), t, \\theta )}{\\partial z}$$ \n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$ \\frac{\\alpha(t + \\delta t)-\\alpha(t)}{\\delta t}  = -\\alpha(t + \\delta t) \\frac{\\partial f(z(t), t, \\theta )}{\\partial z}$$ \n",
    "\n",
    "When $\\lim_{\\delta t\\to 0} $, we have:\n",
    "$$\\frac{\\partial \\alpha(t)}{\\partial t} = \\alpha(t)\\frac{\\partial f(z(t), t, \\theta )}{\\partial z(t)} $$ \n",
    "\n",
    "Integrating from $t_1$ to $t_0$ we have:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z(t)} = \\int_{t_1}^{t_0} \\alpha(t)\\frac{\\partial f(z(t), t, \\theta )}{\\partial z(t)} \\tag {4}$$ \n",
    "\n",
    "Here, the last term of this equation represents the derivative of the \"network\" function, so, it is easily calculable by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third equation derivation (equation $2.3$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Well, the third equation is simply a rewrite of the equation $(2)$, the equation of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
